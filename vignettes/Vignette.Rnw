% \VignetteIndexEntry{An R package for multivariate response generalized linear models.}
% \VignetteDepends{MGLM}
% \VignetteKeyword{Multivariate generalized linear model}
% \VignetteKeyword{Distribution fitting}
% \VignetteKeyword{Regression}
% \VignetteKeyword{Sparse regression}

\documentclass[a4paper]{article}
\usepackage{Sweave}
%\usepackage[cm,headings]{fullpage}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{hypcap}
\usepackage{subfigure}
\usepackage{rotating}
%\usepackage{draftwatermark}
\usepackage{fancyhdr}
\usepackage[section]{placeins}
\usepackage{graphicx} 
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage[space]{grffile} 
\usepackage{multicol}
\usepackage{color}
\title{{\tt MGLM} package vignette}
\author{Yiwen Zhang \and Hua Zhou}

\begin{document}
\SweaveOpts{concordance=TRUE}

\maketitle


The analysis of multivariate count data arises in numerous fields including genomics, image analysis, text mining, and sports analytics.  The multinomial logit model is limiting due to its restrictive mean-variance structure. Moreover, it assumes that counts of different categories are negatively correlated.  Models that allow over-dispersion and possess more flexible positive and/or negative correlation structures offer more realism.  We implement four models in the R package {\tt MGLM}: multinomial logit (MN), Dirichlet multinomial (DM), generalized Dirichlet multinomial (GDM), and negative mutinomial (NegMN). Distribution fitting, regression, hypothesis testing, and variable selection are treated in a unified framework. 

The simulted data we plan to analyze is multivariate count data with $d$ categories.

<<eval=TRUE, echo=FALSE, include=FALSE>>=
require(ggplot2)
require(reshape2)
require(plyr)
@
<<eval=TRUE, echo=TRUE>>=
require(MGLM)
@

Before getting started with model fitting, we explore the correlation structure of the response counts. 

<<cache=TRUE>>=
data(iris)
Y <-iris[, 1:4]
@

<<label=scatter1, echo=FALSE, pdf=FALSE, results=hide, include=TRUE>>=
scatter.plot <- function(Y, main=NULL, free=TRUE, ...){
   "ID" <- "variable1"<-"value1" <- "value" <- NULL
  if(is.matrix(Y)){
    Y <- as.data.frame(Y)
    Y$ID <- rownames(Y)
  }else if(is.data.frame(Y)){
    Y$ID <- row.names(Y)
  }else{
    stop("Please provide a matrix or a data frame.")
  }
  meltDf <- melt(Y, id='ID')
  names(meltDf) <- c("ID", "variable1", "value1")
  out <- ddply(meltDf, .(variable1), function(df, df2){
    excl <- unique(df$variable)
    outDf <- merge(df, df2, by="ID")
  }, Y)
  meltDf2 <- melt(out, id=c("ID", "variable1", "value1"))
  meltDf2$value <- ifelse(meltDf2$variable1==meltDf2$variable, NA, 
                          meltDf2$value)
  meltDf2 <- subset(meltDf2, !is.na(value))
  if(!free){
  plot <- ggplot(meltDf2, aes(x=value1, y=value)) + 
    geom_point() + 
    facet_grid(variable1~variable,as.table=FALSE) +
    xlab("") + ylab("") + ggtitle(main) + 
    theme_bw()+
    theme(axis.text.x=element_text(size=6),
          axis.text.y=element_text(size=6))
  }else{
    plot <- ggplot(meltDf2, aes(x=value1, y=value)) + 
      geom_point() + 
      facet_wrap(variable1~variable,as.table=FALSE, scales="free") +
      xlab("") + ylab("") + ggtitle(main) + 
      theme_bw()+
      theme(axis.text.x=element_text(size=6),
            axis.text.y=element_text(size=6))
  }
  

  return(plot)
}

scatter.plot(Y, free=TRUE)
@

<<label=corr, echo=FALSE, pdf=FALSE, results=hide, include=TRUE>>=
corr.plot <- function(Y, main=NULL){
  "Var1" <- "Var2" <- "value" <- NULL
  df <- melt(cor(Y))
  names(df) <- c(c("Var1", "Var2", "value"))
  plot <- qplot(x=Var1, y=Var2, data=subset(df, Var1!=Var2), 
    fill=factor(as.character(sign(value)), levels=c("-1", "1")), 
    alpha=value, geom="tile")+
    theme_bw()+
    scale_alpha(guide=FALSE)+
    scale_fill_discrete("", drop=FALSE) + 
    ggtitle(main)+
    xlab("") + ylab("")

  return(plot)
}
corr.plot(Y)
@

\begin{figure}
\begin{center}
$$
\begin{array}{cc}
\setkeys{Gin}{height=2in,width=2.3in}
<<label=scatter1,fig=TRUE,echo=FALSE>>=
<<scatter1>>
@
& 
\setkeys{Gin}{height=2in,width=2.7in}
<<label=corr,fig=TRUE,echo=FALSE>>=
<<corr>>
@
\end{array}
$$
\end{center}
\caption{Scatter plot and correlation plot of the multivariate counts.}
\label{fig:scatter}
\end{figure}



\section{Distribution fitting}

The function {\tt MGLMfit} fits various multivariate discrete distributions and outputs a list with the maximum likelihood estimate (MLE) and relavent statistics.  

When fitting distributions, i.e. no covariates involved, MN is a sub-model of DM, and DM is a sub-model of GDM. %We can perform the likelihood ratio test (LRT) to make comparison between the three models. 
{\tt MGLMfit} outputs the p-value of the likelihood ratio test (LRT) for comparing the fitted model with the most commonly used multinomial model.  NegMN model does not have a nesting relationship with any of the other three models. Therefore no LRT is performed when fitting a NegMN distribution.

\subsection{Multinomial (MN)}

We first generate data from a multinomial distribution. Note the multinomial parameter (must be positive) supplied to the `rmn' function is automatically scaled to be a probability vector.
<<cache=TRUE>>=
set.seed(123)
n <- 200
d <- 4
alpha <- rep(1,d)
m <- 50
Y <- rmn(m, alpha, n)
@
Fitting the DM distribution to the multinomial data shows no advantage.
<<cache=TRUE>>=
mnFit <- MGLMfit(Y, dist="DM")
print(mnFit)
@
The DM parameter estimates and their standard errors are both big, indicating possible overfitting by the DM model. This is confirmed by the fact that the p-value of the LRT for comparing MN to DM is close to 1.

\subsection{Dirichlet-multinomial (DM)}

DM is a Dirichlet mixture of multinomials and allows over-dispersion. Same as MN model, it assumes that the counts of any two different categories are negatively correlated. We generate the data from the DM model and fit distribution.
<<cache=TRUE>>=
set.seed(123)
n <- 200
d <- 4
alpha <- rep(1, d)
m <- 50
Y <- rdirm(m, alpha, n)
@
<<cache=TRUE>>=
dmFit <- MGLMfit(Y, dist="DM")
print(dmFit)
@
The estimate is very close to the true value with small standard errors. The LRT shows that the DM model is significantly better than the MN model.

\subsection{Generalized Dirichlet-multinomial (GDM)}

GDM model uses $d-2$ more parameters than the DM model and  allows both positive and negative correlations among categories. DM is a sub-model of GDM. Here we fit a GDM model to the above DM sample.
<<cache=TRUE>>=
gdmFit <- MGLMfit(Y, dist="GDM")
print(gdmFit)
@
GDM yields a slightly larger log-likelihood value but a larger BIC, suggesting DM as a preferred model. %This is confirmed by a formal LRT
% <<cache=TRUE>>=
% lrtGdmvsdm <- pchisq(2*(gdmFit$loglikelihood - dmFit$loglikelihood), d-2, lower.tail=FALSE)
% print(lrtGdmvsdm)
% @
Now we simulate data from GDM and fit the GDM distribution.
<<warning=FALSE, cache=TRUE>>=
set.seed(124)
n <- 200
d <- 4
alpha <- rep(1, d-1)
beta <- rep(1, d-1)
m <- 50
Y <- rgdirm(m, alpha, beta, n)
gdmFit <- MGLMfit(Y, dist="GDM")
print(gdmFit)
@

\subsection{Negative multinomial (NegMN)}

NegMN model is a multivariate extension to the negative binomial model.  It assumes positive correlation among the counts. To generate data from NegMN model and fit the NegMN distribution,
<<cache=TRUE>>=
set.seed(1220)
n <- 100
d <- 4
p <- 5
prob <- rep(0.2, d)
beta <- 10
Y <- rnegmn(prob, beta, n)
negmnFit <- MGLMfit(Y, dist="NegMN")
print(negmnFit)
@

\section{Regression}

In regression, the $n \times p$ covariate matrix $X$ is similar to that used in the {\tt glm} function. The response should be a $n \times d$ count matrix. Unlike estimating a parameter vector $\beta$ in GLM, we need to estimate a parameter matrix $B$ when the responses are multivariate.  The dimension of the parameter matrix depends on the model: 
\begin{itemize}
\item MN:     $p\times (d-1)$
\item DM:     $p\times d$
\item GDM:    $p\times 2(d-1)$
\item NegMN:  $p\times (d+1)$
\end{itemize}

The GDM model provides the most flexibility, but also requires most parameters.  When using function {\tt MGLMreg} to run regression, we can pick the model by specifying the option {\tt dist="MN"}, {\tt "DM"}, {\tt "GDM"} or {\tt "NegMN"}. 

The rows $B_{j,\cdot}$ of the parameter matrix correspond to covariates. By default, the function output the Wald test statistics and p-values for testing $H_0: B_{j,\cdot}={\bf 0}$ vs $H_a: B_{j, \cdot}\neq {\bf 0}$. If specifying the option {\tt LRT=TRUE}, the function also outputs LRT statistics and p-values.

Next we demonstrate that model mis-specification results in failure in hypothesis testing.  We simulate response data from the GDM model. Covariates $X_1$ and $X_2$ have no effect and $X_3$, $X_4$, $X_5$ have impact on the response. 
<<cache=TRUE>>=
set.seed(1234)
n <- 200
p <- 5
d <- 4
X <- matrix(runif(p*n), n, p)
alpha <- matrix(c(0.6, 0.8, 1), p, d-1, byrow=TRUE)
alpha[c(1,2),] <- 0
Alpha <- exp(X%*%alpha)
beta <- matrix(c(1.2, 1, 0.6), p, d-1, byrow=TRUE)
beta[c(1,2),] <- 0
Beta <- exp(X%*%beta)
m <- runif(n, min=0, max=25) + 25
Y <- rgdirm(m, Alpha, Beta)
@
We fit various regression models and test significance of covariates.

\subsection{Multinomial regression}

<<warning=FALSE, cache=TRUE>>=
mnReg <- MGLMreg(Y~0+X, dist="MN")
print(mnReg)
@

The Wald test shows that all predictors are significantly different from $0$, including the null predictors $X_1$ and $X_2$.

\subsection{Dirichlet-multinomial regression}

<<warning=FALSE,cache=TRUE>>=
dmReg <- MGLMreg(Y~0+X, dist="DM")
print(dmReg)
@

Again, Wald test declares all predictors as significant.

\subsection{Generalized Dirichlet-multinomial Regression}

<<cache=TRUE, warning=FALSE>>=
gdmReg <- MGLMreg(Y~0+X, dist="GDM")
print(gdmReg)
@

When using the correct model, Wald test is able to differentiate the null effects from the significant ones. GDM regression yields the highest log-likelihood and smallest BIC.

\subsection{Negative multinomial regression}

<<cache=TRUE, warning=FALSE>>=
negReg <- MGLMreg(Y~0+X, dist="NegMN", regBeta=FALSE)
print(negReg)
@

Again, the Wald test declares all preditors to be significant. 

%' The plot of fitted versis true values can be made easily with 
%' <<label=fit1, echo=TRUE, pdf=FALSE, results=hide,  include=TRUE>>=
%' plot(gdmReg, facet=TRUE, free=TRUE)
%' @
% Faceting display is an option. The free arguement controls whether to use shared scale across all facets.

\subsection{Prediction}

We can use the fitted model to make prediction. The output of the prediction function is the probabilities of the $d$ categories.  This helps answer questions such as whether certain features increase the probability of observing category $j$. Take the fitted GDM model as an example:
<<cache=TRUE>>=
newX <- matrix(runif(1*p), 1, p)
pred <- predict(gdmReg, newX)
pred
@

\section{Sparse regression}

Regularization is an important tool for model selection and improving the risk property of the estimates.  In the package, we implemented three types of penalties on the paramter matrix $B$:
\begin{itemize}
\item select by entries
\item select by rows
\item select by rank
\end{itemize}

The function {\tt MGLMtune} finds the optimal tuning parameter with smallest BIC and outputs the estimate using the chosen tuning parameter.  The output from {\tt MGLMtune} is a list containing the solution path and the final estimate. Users can either provide a vector of tuning parameters with option {\tt lambdas} or specify the number of grid points via option {\tt ngridpt} and let the function decide the default tuning parameters. The function {\tt MGLMsparsereg} computes the regularized estimate at a given tuning paramter value {\tt lambda}.

We generate the data from the DM model, with row sparsity, and show how each penalty type works. 

<<cache=TRUE >>=
set.seed(118)
n <- 100
p <- 10
d <- 5
m <- rbinom(n, 200, 0.8)
X <- matrix(rnorm(n*p),n, p)
alpha <- matrix(0, p, d)
alpha[c(1,3, 5), ] <- 1
Alpha <- exp(X%*%alpha)
Y <- rdirm(size=m, alpha=Alpha)
@

\subsection{Select by entries}

<<warning=FALSE, cache=FALSE>>=
sweep <- MGLMtune(Y~0+X, dist="DM", penalty="sweep", ngridpt=30)
print(sweep$select)
@


%' <<label=sweeppath, echo=TRUE, pdf=FALSE, results=hide,  include=TRUE>>=
%' plot(sweep)
%' @
%' 
%' \begin{figure}
%' \begin{center}
%' \setkeys{Gin}{height=2.70in,width=2.15in}
%' <<fig=TRUE, echo=FALSE>>=
%' <<sweeppath>>
%' @
%' \end{center}
%' \caption{Variable selection by entries}
%' \label{fig:lasso-solpath}
%' \end{figure}


\subsection{Select by rows}

Since the rows of the parameter matrix correspond to predictors, selecting by rows performs variable selection at the predictor level. 

<<warning=FALSE, cache=FALSE>>=
group <- MGLMtune(Y~0+X, dist="DM", penalty="group", ngridpt=30)
print(group$select)
@


%' <<label=grouppath, echo=TRUE,  pdf=FALSE, results=hide, include=TRUE>>=
%' plot(group)
%' @
%' 
%' \begin{figure}
%' \begin{center}
%' \setkeys{Gin}{height=2.70in,width=2.15in}
%' <<fig=TRUE, echo=FALSE>>=
%' <<grouppath>>
%' @
%' \end{center}
%' \caption{Variable selection by groups}
%' \label{fig:group-solpath}
%' \end{figure}


\subsection{Select by singular values}

Nuclear norm regularization encourages low rank in the regularized estimate. 

<<warning=FALSE, cache=FALSE>>=
nuclear <- MGLMtune(Y~0+X, dist="DM", penalty="nuclear", ngridpt=30, warm.start=FALSE)
print(nuclear$select)
@

%' <<label=nuclearpath, echo=TRUE, pdf=FALSE, results=hide, include=TRUE>>=
%' plot(nuclear)
%' @
%' 
%' 
%' 
%' \begin{figure}
%' \begin{center}
%' \setkeys{Gin}{height=2.70in,width=2.15in}
%' <<fig=TRUE, echo=FALSE>>=
%' <<nuclearpath>>
%' @
%' \end{center}
%' \caption{Variable selection by singular values}
%' \label{fig:nuclear-solpath}
%' \end{figure}

\end{document}